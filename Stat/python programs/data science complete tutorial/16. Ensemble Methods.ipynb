{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods\n",
    "### Agenda\n",
    "\n",
    "<hr>\n",
    "1. Introduction to Ensemble Methods\n",
    "2. RandomForest\n",
    "3. AdaBoost\n",
    "4. GradientBoostingTree\n",
    "5. VotingClassifier\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction to Ensemble Method\n",
    "* Objective of ensemble methods is to combine the predictions of serveral base estimators ( Linear Regression, Decisison Tree, etc. ) to create a combined effect or more genralized model.\n",
    "* Two types of Ensemble Method\n",
    "  - Averaging Method : Build several estimators independently & average their predictions. Examples are RandomForest etc.\n",
    "  - Boosting Method : Base estimators are built sequentially using weighted version of data .i.e fitting models with data that were mis-classified. Examples are AdaBoost\n",
    "  \n",
    "<img src=\"https://cdn-images-1.medium.com/max/1000/1*PaXJ8HCYE9r2MgiZ32TQ2A.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. RandomForest\n",
    "* Recap - Limitations of decison tree is that it overfits & shows high variance.\n",
    "* RandomForest is an averaging ensemble method whose prediction is function of prediction of 'n' decision trees.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Stavros_Dimitriadis/publication/324517994/figure/fig1/AS:615965951799303@1523869135381/Classification-process-based-on-the-Random-Forest-algorithm-2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Algorithm\n",
    "* Data consist of R rows & M features.\n",
    "* Sample of training data is taken.\n",
    "* Random set of features are selected.\n",
    "* As many as configured number of trees are created using above two steps.\n",
    "* Final prediction in case of classification is majority prediction.\n",
    "* Final prediction in case of regression is mean/median of individual tree prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparing Decision Tree & Random Forest for DIGITS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=7, min_samples_split=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=7, min_samples_split=3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.fit(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.896807720861173"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.score(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.score(testX,testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=200, max_features=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_features=0.2, n_estimators=200)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9755555555555555"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(testX,testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Important Hyper-parameters\n",
    "* n_estimators : number of trees to be configured, larger is better but compute cost.\n",
    "* max_features : maximum number of features to be considered for splitting the node. For classification this equals to sqrt(n_features). And, for regression max_features = n_features.\n",
    "* n_jobs : Configure as -1 so that we can make use of all cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages\n",
    "* Minimal data cleaning or dealing with missing values required.\n",
    "* Works well with high dimensional datasets\n",
    "* Minimizes variance even for low variance models\n",
    "* RandomForest can tell importance of features. We can find important features & use them in model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 1.54920035e-03, 1.96870590e-02, 9.35236424e-03,\n",
       "       8.41091370e-03, 2.10345818e-02, 6.18854059e-03, 5.84559056e-04,\n",
       "       7.05870461e-05, 8.00205775e-03, 2.65603876e-02, 5.42083673e-03,\n",
       "       1.65880170e-02, 2.86320512e-02, 4.02825531e-03, 4.44051462e-04,\n",
       "       4.80112129e-05, 6.82206626e-03, 1.80035888e-02, 2.82765365e-02,\n",
       "       2.94024007e-02, 5.59203051e-02, 8.74802223e-03, 3.37947947e-04,\n",
       "       8.04795379e-05, 1.22489555e-02, 4.55732556e-02, 2.82321414e-02,\n",
       "       3.91189238e-02, 2.38081048e-02, 2.86415029e-02, 3.16841259e-05,\n",
       "       0.00000000e+00, 3.07430774e-02, 2.42844529e-02, 1.80528568e-02,\n",
       "       4.11147140e-02, 1.74967377e-02, 2.40216770e-02, 0.00000000e+00,\n",
       "       2.29877154e-05, 8.56817721e-03, 3.97073526e-02, 4.64872145e-02,\n",
       "       2.33674902e-02, 2.10727872e-02, 1.77071122e-02, 8.25468820e-06,\n",
       "       4.19917094e-05, 2.23616031e-03, 1.49014237e-02, 1.85877706e-02,\n",
       "       1.32503699e-02, 2.92380313e-02, 2.30424436e-02, 1.29995036e-03,\n",
       "       7.16414273e-05, 1.45244534e-03, 1.82525169e-02, 9.58649486e-03,\n",
       "       2.63033005e-02, 2.94501735e-02, 1.45424049e-02, 3.24059994e-03])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.n_outputs_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. AdaBoost\n",
    "* Boosting in general is about building a model from the training data, then creating a second model that attempts to correct the errors from the first model. Models are added until the training set is predicted perfectly or a maximum number of models are added.\n",
    "* AdaBoost was first boosting algorithm.\n",
    "* AdaBoost can be used for both classification & regression\n",
    "\n",
    "##### Algorithm\n",
    "* Core concept of adaboost is to fit weak learners ( like decision tree ) sequantially on repeatedly modifying data.\n",
    "* Initially, each data is assigned equal weights.\n",
    "* A base estimator is fitted with this data.\n",
    "* Weights of misclassified data are increased & weights of correctly classified data is decreased. \n",
    "* Repeat the above two steps till all data are correctly classified or max number of iterations configured.\n",
    "* Making Prediction : The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=8),n_estimators=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=8),\n",
       "                   n_estimators=600)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab.fit(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9866666666666667"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab.score(testX,testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = AdaBoostClassifier(base_estimator=RandomForestClassifier(n_estimators=20),n_estimators=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=RandomForestClassifier(n_estimators=20),\n",
       "                   n_estimators=600)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab.fit(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9733333333333334"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab.score(testX,testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. GradientBoostingTree\n",
    "* A machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.\n",
    "* One of the very basic assumption of linear regression is that it's sum of residuals is 0.\n",
    "* These residuals as mistakes committed by our predictor model. \n",
    "* Although, tree based models are not based on any of such assumptions, but if sum of residuals is not 0, then most probably there is some pattern in the residuals of our model which can be leveraged to make our model better. \n",
    "* So, the intuition behind gradient boosting algorithm is to leverage the pattern in residuals and strenghten a weak prediction model, until our residuals don't show any pattern.\n",
    "* Algorithmically, we are minimizing our loss function, such that test loss reach it’s minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem : House Price Prediction using GradientBoostingTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_data = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = house_data.data\n",
    "y = house_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt.fit(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = np.zeros(100, dtype=np.float64)\n",
    "for i, y_pred in enumerate(gbt.staged_predict(testX)):\n",
    "    test_score[i] = gbt.loss_(testY, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Least squares Loss')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_score)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Least squares Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. VotingClassifier\n",
    "* Core concept of VotingClassifier is to combine conceptually different machine learning classifiers and use a majority vote or weighted vote to predict the class labels. \n",
    "* Voting classifier is quite effective with good estimators & handles individual's limitations, ensemble methods can also participate. \n",
    "* Types of Voting Classifier\n",
    "  - Soft Voting Classifier, different weights configured to different estimator\n",
    "  - Hard Voting Classifier, all estimators have equal weighage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem : DIGIT identification using VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier,RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [ \n",
    "    ('rf',RandomForestClassifier(n_estimators=20)),\n",
    "    ('svc',SVC(kernel='rbf', probability=True)),\n",
    "    ('knc',KNeighborsClassifier()),\n",
    "    ('abc',AdaBoostClassifier(base_estimator=DecisionTreeClassifier() ,n_estimators=20)),\n",
    "    ('lr',LogisticRegression()) \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = VotingClassifier(estimators=estimators, voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = digits.data, digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soura\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('rf', RandomForestClassifier(n_estimators=20)),\n",
       "                             ('svc', SVC(probability=True)),\n",
       "                             ('knc', KNeighborsClassifier()),\n",
       "                             ('abc',\n",
       "                              AdaBoostClassifier(base_estimator=DecisionTreeClassifier(),\n",
       "                                                 n_estimators=20)),\n",
       "                             ('lr', LogisticRegression())])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc.fit(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9866666666666667"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc.score(testX,testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf 0.9511111111111111\n",
      "svc 0.9911111111111112\n",
      "knc 0.9866666666666667\n",
      "abc 0.8377777777777777\n",
      "lr 0.9711111111111111\n"
     ]
    }
   ],
   "source": [
    "for est,name in zip(vc.estimators_,vc.estimators):\n",
    "    print (name[0], est.score(testX,testY))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = VotingClassifier(estimators=estimators, voting='soft', weights=[2,.1,3,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soura\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('rf', RandomForestClassifier(n_estimators=20)),\n",
       "                             ('svc', SVC(probability=True)),\n",
       "                             ('knc', KNeighborsClassifier()),\n",
       "                             ('abc',\n",
       "                              AdaBoostClassifier(base_estimator=DecisionTreeClassifier(),\n",
       "                                                 n_estimators=20)),\n",
       "                             ('lr', LogisticRegression())],\n",
       "                 voting='soft', weights=[2, 0.1, 3, 2, 2])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc.fit(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc.score(testX,testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
